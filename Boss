with tab3:
    st.subheader("TAB 3 — PCA-only + DV01 constituant (β fixe) + Cross-hedge XCCY")

    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt
    from hmmlearn.hmm import GaussianHMM
    from sklearn.preprocessing import StandardScaler
    from sklearn.cluster import KMeans
    from sklearn.decomposition import PCA

    # =========================================================
    # A) PARAMÈTRES GÉNÉRAUX
    # =========================================================
    c1, c2, c3, c4 = st.columns(4)
    with c1:
        hmm_states = st.slider("Nb régimes (HMM – display seulement)", 2, 6, 3, 1, key="tab3_hmm_states")
    with c2:
        n_iter = st.number_input("n_iter (EM HMM)", 50, 2000, 300, 50, key="tab3_hmm_niter")
    with c3:
        topN = st.slider("Top N constituants / jour", 1, 20, 5, 1, key="tab3_topN")
    with c4:
        apply_day = st.selectbox("Appliquer le hedge", ["Jour T", "Jour T+1"], index=1, key="tab3_apply_day")

    d1, d2, d3 = st.columns(3)
    with d1:
        tau1 = st.slider("Seuil τ1 (→ α=0.3)", 0.0, 1.0, 0.50, 0.05, key="tab3_tau1")
    with d2:
        tau2 = st.slider("Seuil τ2 (→ α=0.6)", 0.0, 1.0, 0.70, 0.05, key="tab3_tau2")
    with d3:
        tau3 = st.slider("Seuil τ3 (→ α=1.0)", 0.0, 1.0, 0.85, 0.05, key="tab3_tau3")

    e1, e2, e3 = st.columns(3)
    with e1:
        cost_rate = st.number_input("Coût — Rates (par DV01 own)", 0.0, 5.0, 0.20, 0.01, key="tab3_cost_rate")
    with e2:
        cost_xccy = st.number_input("Coût — XCCY (par DV01 own)", 0.0, 5.0, 0.25, 0.01, key="tab3_cost_xccy")
    with e3:
        cost_fx   = st.number_input("Coût — FX (par FX01 own)",   0.0, 5.0, 0.00, 0.01, key="tab3_cost_fx")

    # =========================================================
    # B) CLUSTERS v2 — PAR BUCKET (RATES / XCCY / FX)
    # =========================================================
    st.markdown("### Clusters de facteurs (KMeans par bucket)")

    cl1, cl2, cl3 = st.columns([2,1,1])
    with cl1:
        use_clusters = st.checkbox("Activer clusters KMeans par bucket", value=True, key="tab3_use_clusters")
    with cl2:
        n_clusters = st.slider("Nb max de clusters par bucket", 1, 8, 3, 1, key="tab3_n_clusters")
    with cl3:
        max_1_per_cluster = st.checkbox("Max 1 hedge / cluster / jour", value=True, key="tab3_1_per_cluster")

    def _bucket(name: str) -> str:
        u = str(name).upper()
        if "FX" in u or ("FX_COLS" in globals() and name in FX_COLS):
            return "FX"
        if "XCCY" in u or "BASIS" in u:
            return "XCCY"
        return "Rates"

    factors = list(pnl_by_factor.columns)
    buckets = pd.Series({c: _bucket(c) for c in factors}, name="Bucket")

    cluster_map = None

    if use_clusters:
        corr_mat = pnl_by_factor.diff().corr().fillna(0.0)
        cluster_labels = pd.Series(index=factors, dtype="float")
        current_cluster_id = 1

        for bkt in ["Rates", "XCCY", "FX"]:
            cols_b = [c for c in factors if buckets[c] == bkt]
            if not cols_b:
                continue
            corr_b = corr_mat.loc[cols_b, cols_b]
            k_b = min(int(n_clusters), len(cols_b))
            if k_b <= 1:
                for c in cols_b:
                    cluster_labels[c] = current_cluster_id
                current_cluster_id += 1
                continue
            km_b = KMeans(n_clusters=k_b, random_state=42, n_init="auto")
            labels_b = km_b.fit_predict(corr_b.values)
            for c_name, lab in zip(cols_b, labels_b):
                cluster_labels[c_name] = current_cluster_id + lab
            current_cluster_id += k_b

        cluster_map = cluster_labels.astype(int)

        cluster_df = pd.concat([cluster_map.rename("Cluster"), buckets], axis=1).sort_values(["Bucket","Cluster"])
        sorted_idx = cluster_df.index
        sorted_corr = corr_mat.loc[sorted_idx, sorted_idx]

        with st.expander("Vue clusters (Bucket + Cluster)", expanded=False):
            st.dataframe(cluster_df, use_container_width=True, height=300)

        with st.expander("Heatmap ΔPnL triée par bucket & cluster", expanded=False):
            figC, axC = plt.subplots(figsize=(6,5), dpi=120)
            im = axC.imshow(sorted_corr.values, aspect="auto")
            axC.set_xticks(range(len(sorted_idx)))
            axC.set_yticks(range(len(sorted_idx)))
            axC.set_xticklabels(sorted_idx, rotation=90, fontsize=6)
            axC.set_yticklabels(sorted_idx, fontsize=6)
            axC.set_title("Corrélation ΔPnL triée par Bucket & Cluster")
            figC.colorbar(im, ax=axC, fraction=0.046, pad=0.04)
            st.pyplot(figC)
    else:
        cluster_map = None

    # =========================================================
    # C) HMM MULTIVARIÉ — DISPLAY SEULEMENT
    # =========================================================
    st.markdown("### Régimes HMM (display – ne drive pas le hedge)")

    X_df = pnl_by_factor.replace([np.inf, -np.inf], np.nan).dropna(how="any")
    idx_obs = X_df.index

    scaler = StandardScaler()
    X_use = scaler.fit_transform(X_df.values)

    hmm = GaussianHMM(
        n_components=int(hmm_states),
        covariance_type="full",
        n_iter=int(n_iter),
        random_state=42
    )
    hmm.fit(X_use)

    states = pd.Series(hmm.predict(X_use), index=idx_obs, name="state")
    post = pd.DataFrame(
        hmm.predict_proba(X_use),
        index=idx_obs,
        columns=[f"Regime {i}" for i in range(int(hmm_states))]
    )

    net_on_state = pd.concat([net_daily.reindex(idx_obs), states], axis=1).dropna()
    means_by_state = net_on_state.groupby("state")["NetDailyPnL"].mean()
    stress_regime = int(means_by_state.idxmin()) if len(means_by_state) > 0 else 0

    hmm_proba_stress = post.iloc[:, stress_regime].rename("hmm_proba_stress")

    figR, axR = plt.subplots(figsize=(12,4), dpi=120)
    axR.plot(net_daily.index, net_daily.values, lw=1.1, label="PnL quotidien")
    cmap = plt.colormaps.get_cmap("tab10")
    for s_id in range(int(hmm_states)):
        mask = (states == s_id).reindex(net_daily.index).fillna(False)
        axR.fill_between(net_daily.index, net_daily.values, 0,
                         where=mask, alpha=0.12, label=f"Régime {s_id}", color=cmap(s_id))
    auto_xticks(axR, net_daily.index)
    format_yaxis_plain(axR)
    axR.set_title("HMM multivarié — Régimes & PnL (info)")
    axR.legend(loc="upper left", ncols=2)
    st.pyplot(figR)

    # =========================================================
    # D) SIGNAL PCA-ONLY PAR CLUSTER → α_df
    # =========================================================
    st.markdown("### Signal : PCA cluster-only → α")

    eps = 1e-12
    base_index = pnl_by_factor.index

    pca_stress_cluster = None
    if use_clusters and (cluster_map is not None):
        cl_map = cluster_map.reindex(factors).dropna()
        cluster_ids = sorted(int(c) for c in cl_map.unique())
        pca_stress_cluster = pd.DataFrame(0.0, index=base_index, columns=cluster_ids)

        for cid in cluster_ids:
            cols_c = cl_map[cl_map == cid].index.tolist()
            if len(cols_c) < 2:
                continue
            X_c = pnl_by_factor[cols_c].diff().dropna(how="any")
            if len(X_c) < 30:
                continue

            pca = PCA(n_components=1, random_state=42)
            scores = pca.fit_transform(X_c.values).ravel()
            scores = pd.Series(scores, index=X_c.index)

            z = (scores - scores.mean()) / (scores.std(ddof=1) + eps)
            stress = (z.abs() / 3.0).clip(0.0, 1.0)

            pca_stress_cluster[cid] = stress.reindex(base_index).ffill().fillna(0.0)
    else:
        pca_stress_cluster = None

    # PCA-stress au niveau facteur
    if use_clusters and (cluster_map is not None) and (pca_stress_cluster is not None):
        cl_map_full = cluster_map.reindex(factors)
        cluster_ids = list(pca_stress_cluster.columns)
        pca_stress_factor = pd.DataFrame(0.0, index=base_index, columns=factors)
        for c in factors:
            cid = cl_map_full.get(c, np.nan)
            if not np.isnan(cid) and (cid in cluster_ids):
                pca_stress_factor[c] = pca_stress_cluster[int(cid)]
            else:
                pca_stress_factor[c] = 0.0
    else:
        pca_stress_factor = pd.DataFrame(0.0, index=base_index, columns=factors)

    # score = juste PCA
    score_combined = pca_stress_factor.copy()

    def map_to_alpha(series_like: pd.DataFrame) -> pd.DataFrame:
        out = pd.DataFrame(index=series_like.index, columns=series_like.columns, dtype=float)
        out[series_like <  tau1] = 0.0
        out[(series_like >= tau1) & (series_like <  tau2)] = 0.3
        out[(series_like >= tau2) & (series_like <  tau3)] = 0.6
        out[series_like >= tau3] = 1.0
        return out

    alpha_df = map_to_alpha(score_combined).fillna(0.0)
    alpha_global = None  # pas utilisé

    # =========================================================
    # E) BETA FIXE (par constituant) & PROXY CROSS-HEDGE XCCY
    # =========================================================
    st.markdown("### Bêta fixe vs data type + proxy XCCY")

    betas = {}
    dF = pnl_by_factor.diff().replace([np.inf, -np.inf], np.nan).dropna()
    dD = mkt_change[dtype].reindex(dF.index).fillna(0.0)

    for c in factors:
        y = dF[c]
        x = dD
        df_xy = pd.concat([x, y], axis=1).dropna()
        if len(df_xy) < 30:
            betas[c] = 1.0
            continue
        X = df_xy.iloc[:, 0].values
        Y = df_xy.iloc[:, 1].values
        denom = (X**2).sum() + eps
        beta = (X * Y).sum() / denom
        betas[c] = beta

    # proxy pour cross-hedge si dtype est XCCY et presente dans pnl_by_factor
    best_proxy_for_dtype = None
    if _bucket(dtype) == "XCCY" and dtype in dF.columns:
        corr_series = dF.corrwith(dF[dtype]).drop(index=dtype).dropna()
        if not corr_series.empty:
            # filtrer uniquement proxies Rates / FX
            allowed = [c for c in corr_series.index if _bucket(c) in ("Rates", "FX")]
            corr_allowed = corr_series.loc[allowed] if allowed else pd.Series(dtype=float)
            if not corr_allowed.empty:
                corr_abs = corr_allowed.abs()
                # seuil 40%
                corr_filtered = corr_abs[corr_abs >= 0.40]
                if not corr_filtered.empty:
                    best_proxy_for_dtype = corr_filtered.idxmax()

    # =========================================================
    # F) BOUCLE HEDGE — DV01 constituant (own) + cross-hedge XCCY
    # =========================================================
    st.markdown("### Plan de hedge (DV01 constituant + cross-hedge XCCY)")

    base_index = pnl_by_factor.index
    risk_ts_loc = (
        risk_ts.reindex(base_index)
               .reindex(columns=factors)
    )

    cost_series = pd.Series(0.0, index=base_index)
    plan_rows = []

    for d in base_index:

        alpha_row = alpha_df.loc[d].reindex(factors).fillna(0.0)
        dv01_dtype_row = risk_ts_loc.loc[d].reindex(factors).fillna(0.0)

        # sélection des top constituants
        if use_clusters and (cluster_map is not None):
            cl_loc = cluster_map.reindex(factors).dropna()
            local_score = (alpha_row.abs() * dv01_dtype_row.abs()).replace([np.inf, -np.inf], 0.0)

            cluster_scores = {}
            for cl_id in cl_loc.unique():
                memb = cl_loc[cl_loc == cl_id].index
                cluster_scores[int(cl_id)] = float(local_score[memb].max())

            all_candidates = local_score.sort_values(ascending=False).index.tolist()
            top_names = []
            used_clusters = set()
            for name in all_candidates:
                if len(top_names) >= int(topN):
                    break
                cid = cl_loc.get(name, None)
                if cid is None:
                    continue
                cid = int(cid)
                if cluster_scores.get(cid, 0.0) <= 0.0:
                    continue
                if max_1_per_cluster and (cid in used_clusters):
                    continue
                if alpha_row.get(name, 0.0) <= 0.0:
                    continue
                top_names.append(name)
                used_clusters.add(cid)
        else:
            local_score = (alpha_row.abs() * dv01_dtype_row.abs()).replace([np.inf, -np.inf], 0.0)
            top_names = (
                local_score.sort_values(ascending=False)
                           .head(int(topN))
                           .index
                           .tolist()
            )

        target_day = d if apply_day == "Jour T" else d + pd.Timedelta(days=1)
        if target_day not in base_index:
            continue

        for c_target in top_names:

            a_j = float(alpha_row.get(c_target, 0.0))
            dv01_c_dtype = float(dv01_dtype_row.get(c_target, 0.0))
            if a_j <= 0.0 or not np.isfinite(dv01_c_dtype) or dv01_c_dtype == 0.0:
                continue

            used_proxy = c_target
            beta_used = betas.get(c_target, 1.0)

            # --- Cas spécial : data type XCCY & constituant = data type
            if (_bucket(dtype) == "XCCY") and (c_target == dtype) and (best_proxy_for_dtype is not None):
                used_proxy = best_proxy_for_dtype
                beta_used = betas.get(used_proxy, 1.0)

            if abs(beta_used) < 1e-6:
                beta_used = 1.0  # sécurité

            # DV01 du proxy en unité "own" (swap, XCCY, FX), équivalent à DV01_const_dtype
            dv01_proxy_own = dv01_c_dtype / beta_used

            # Taille du hedge en DV01 proxy own
            hedge_units_own = - a_j * dv01_proxy_own

            # Exposition vs data type ajoutée par ce hedge
            hedge_units_dtype = hedge_units_own * beta_used  # => -a_j * DV01_const_dtype

            bkt = _bucket(used_proxy)
            unit_cost = cost_fx if bkt == "FX" else (cost_xccy if bkt == "XCCY" else cost_rate)

            if np.isfinite(hedge_units_own):
                cost_series.loc[target_day] += abs(hedge_units_own) * unit_cost

            plan_rows.append({
                "SignalDate": d,
                "Date": target_day,
                "Constituant_target": c_target,
                "alpha": a_j,
                "DV01_const_dtype": dv01_c_dtype,
                "Proxy": used_proxy,
                "Beta_proxy_vs_dtype": beta_used,
                "DV01_proxy_own": dv01_proxy_own,
                "HedgeUnits_own": hedge_units_own,
                "HedgeUnits_dtype": hedge_units_dtype,
                "Bucket": bkt,
                "UnitCost": unit_cost,
                "CostAlloc": abs(hedge_units_own) * unit_cost
            })

    plan_df = pd.DataFrame(plan_rows)

    if plan_df.empty:
        st.info("Aucun hedge déclenché (α=0 partout ou seuils trop élevés).")
    else:
        st.dataframe(
            plan_df.sort_values(["Date","Constituant_target","Proxy"])
                   .style.format({
                       "alpha": "{:.2f}",
                       "DV01_const_dtype": "{:,.0f}",
                       "Beta_proxy_vs_dtype": "{:.3f}",
                       "DV01_proxy_own": "{:,.0f}",
                       "HedgeUnits_own": "{:,.0f}",
                       "HedgeUnits_dtype": "{:,.0f}",
                       "UnitCost": "{:,.3f}",
                       "CostAlloc": "{:,.0f}",
                   }),
            use_container_width=True,
            height=420
        )

    # =========================================================
    # G) PNL — BASE vs HEDGÉ (tout en DV01 vs data type)
    # =========================================================
    st.markdown("### PnL — Base vs Hedgé")

    E_base = (
        cum_risk
        .reindex(base_index)
        .reindex(columns=factors)
        .fillna(0.0)
    )

    driver_series = (
        mkt_change[dtype]
        .reindex(base_index)
        .fillna(0.0)
    )

    pnl_base = (E_base.shift(1).mul(driver_series, axis=0)).sum(axis=1)
    pnl_base = pnl_base.rename("PnL_baseline")

    if plan_df.empty:
        H_dtype = pd.DataFrame(0.0, index=base_index, columns=factors)
    else:
        hedge_delta = (
            plan_df.groupby(["Date","Proxy"])["HedgeUnits_dtype"]
                   .sum()
                   .unstack(fill_value=0.0)
                   .reindex(base_index, fill_value=0.0)
        )
        hedge_delta = hedge_delta.reindex(columns=factors, fill_value=0.0)
        H_dtype = hedge_delta.cumsum()

    E_tot = (E_base.add(H_dtype, fill_value=0.0)).shift(1).fillna(0.0)

    pnl_after = (
        (E_tot.mul(driver_series, axis=0)).sum(axis=1)
        - cost_series.reindex(base_index).fillna(0.0)
    )
    pnl_after = pnl_after.rename("PnL_after")

    cum_base = pnl_base.cumsum().rename("Cum_Base")
    cum_after = pnl_after.cumsum().rename("Cum_Hedged")

    figF, axF = plt.subplots(figsize=(12,4), dpi=120)
    axF.plot(cum_base.index, cum_base.values, label="Cumul Base (Tab1)")
    axF.plot(cum_after.index, cum_after.values, label="Cumul Hedgé (Tab3)")
    auto_xticks(axF, cum_base.index)
    format_yaxis_plain(axF)
    axF.set_title("Cumulative PnL — Base vs Hedgé")
    axF.legend(loc="upper left")
    st.pyplot(figF)

    k1, k2, k3 = st.columns(3)
    with k1:
        st.metric("Δ Cumul (Hedgé - Base)",
                  f"{float(cum_after.iloc[-1]-cum_base.iloc[-1]):,.0f}")
    with k2:
        st.metric("Coût total du hedge", f"{float(cost_series.sum()):,.0f}")
    with k3:
        pct_days = 100.0 * float((alpha_df.max(axis=1) > 0).mean())
        st.metric("% jours couverts", f"{pct_days:.1f}%")

    # =========================================================
    # H) RÉSUMÉ PCA / HEDGE PAR CLUSTER
    # =========================================================
    with st.expander("Résumé PCA par cluster", expanded=False):

        if not (use_clusters and (cluster_map is not None) and (pca_stress_cluster is not None)):
            st.info("Clusters ou PCA-stress non disponibles.")
        else:
            cl_map = cluster_map.dropna().astype(int)
            cluster_ids = sorted(int(c) for c in pca_stress_cluster.columns)

            rows = []
            for cid in cluster_ids:
                cols_c = cl_map[cl_map == cid].index.tolist()
                if not cols_c:
                    continue

                stress_c = pca_stress_cluster[cid].dropna()
                if stress_c.empty:
                    continue

                mean_stress = float(stress_c.mean())
                p95_stress  = float(stress_c.quantile(0.95))
                pct_stress_high = float((stress_c > 0.7).mean() * 100.0)

                alpha_c = alpha_df.reindex(columns=cols_c).fillna(0.0)
                pct_days_hedged = float((alpha_c.max(axis=1) > 0).mean() * 100.0)

                rows.append({
                    "Cluster": cid,
                    "Nb_facteurs": len(cols_c),
                    "Stress_moyen": mean_stress,
                    "Stress_p95": p95_stress,
                    "% jours stress>0.7": pct_stress_high,
                    "% jours hedgés": pct_days_hedged,
                })

            if not rows:
                st.write("Aucun cluster avec stress PCA exploitable.")
            else:
                summary_df = (
                    pd.DataFrame(rows)
                      .set_index("Cluster")
                      .sort_values("Stress_moyen", ascending=False)
                )
                st.dataframe(
                    summary_df.style.format({
                        "Stress_moyen": "{:.2f}",
                        "Stress_p95": "{:.2f}",
                        "% jours stress>0.7": "{:.1f}",
                        "% jours hedgés": "{:.1f}",
                    }),
                    use_container_width=True,
                    height=320
                )
